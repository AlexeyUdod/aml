{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dll.5.hw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyUdod/aml/blob/master/dll_5_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvn2iEWS18E8"
      },
      "source": [
        "###Задание 1.\n",
        "\n",
        "Обучите нейронную сеть решать шифр цезаря.\n",
        "Что надо сделать:\n",
        "1.Написать алгоритм шифра цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
        "2.Сделать нейронную сеть\n",
        "3.Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
        "4.Проверить качество\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXPes88uq3ir"
      },
      "source": [
        "import torch as tr\n",
        "import re\n",
        "from itertools import product\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "t = tr.tensor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4oNmJgnP2Xj",
        "outputId": "9f81911d-7c04-4f8a-af8c-e55d3ebb1a29"
      },
      "source": [
        "dev = tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
        "if dev.type == 'cuda':\n",
        "    print(tr.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(tr.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(tr.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
        "else:\n",
        "    print('CPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDlUKrd1HRv"
      },
      "source": [
        "def i2l(text, dic): \n",
        "    return [''.join([dic[letter.item()] for letter in sentense]) for sentense in text]\n",
        "\n",
        "def l2i(text, dic): \n",
        "    adic = {v:k  for k, v in dic.items()}\n",
        "    return tr.tensor([[adic[letter] for letter in sentense] for sentense in text])\n",
        "\n",
        "def text2cesar(text, shift, dic):\n",
        "    n_classes = len(dic)\n",
        "    return ''.join(i2l((l2i(text,dic) + shift) % n_classes, dic))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuQ-A2SaH8Uq"
      },
      "source": [
        "class rnn1(tr.nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, n_classes, num_layers):\n",
        "        super().__init__()\n",
        "        self.emb = tr.nn.Embedding(n_classes, emb_size)\n",
        "        self.rnn = tr.nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.lin = tr.nn.Linear(hidden_size, n_classes)\n",
        "        self.hidden_size = hidden_size\n",
        "    def forward(self, input):\n",
        "        r = self.emb(input)\n",
        "        r = self.rnn(r)[0]\n",
        "        r = self.lin(r).squeeze()\n",
        "        return r"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC_U8UNFO75T",
        "outputId": "623f0546-68ce-48a8-d914-c6eecee16afc"
      },
      "source": [
        "ab = 'abcdefghijklmnopqrstuvwxyz ,.'\n",
        "dic = dict(zip(range(len(ab)), ab))\n",
        "\n",
        "n_classes = len(dic)\n",
        "cesar_shift = 4\n",
        "seq_len = 1000\n",
        "batch_size = 1\n",
        "n_iters = 1000\n",
        "\n",
        "model = rnn1(30, 100, len(dic), 1).to(dev)\n",
        "criterion = tr.nn.CrossEntropyLoss()\n",
        "optimizer = tr.optim.Adam(model.parameters())\n",
        "loss_best = 10**10\n",
        "for iter in range(n_iters):\n",
        "    text = tr.randint(0, n_classes, (batch_size, seq_len)).to(dev)\n",
        "    cesar = (text + cesar_shift) % n_classes\n",
        "    # optimizer.zero_grad()\n",
        "    pred = model(cesar)\n",
        "    loss = criterion(pred, text.flatten())#.to(dev)\n",
        "    if loss < loss_best: \n",
        "        model_best = copy.copy(model)\n",
        "        loss_best = loss\n",
        "    if iter % (n_iters / 10) == 0: \n",
        "        print(f'iter {iter} loss = {loss} ')\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 loss = 3.35101318359375 \n",
            "iter 100 loss = 0.04205457121133804 \n",
            "iter 200 loss = 0.0015210752608254552 \n",
            "iter 300 loss = 0.00038793127168901265 \n",
            "iter 400 loss = 8.410885493503883e-05 \n",
            "iter 500 loss = 0.0002120143617503345 \n",
            "iter 600 loss = 4.385572538012639e-05 \n",
            "iter 700 loss = 0.0040789758786559105 \n",
            "iter 800 loss = 0.04842698201537132 \n",
            "iter 900 loss = 0.05320155248045921 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWMw4r1qeIf8",
        "outputId": "339e301f-2e3a-4500-b7dc-ac73f825a183"
      },
      "source": [
        "loss_best"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.5994e-06, device='cuda:0', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H8nlzR6GFYo",
        "outputId": "261cffa3-64d9-4a6e-887a-d564e07e3ee7"
      },
      "source": [
        "text = '''Las Vegas Last April, John took a trip to Las Vegas, Nevada. \n",
        "Las Vegas is a popular destination in the western portion of the United States. \n",
        "The town is most popular for its casinos, hotels, and exciting nightlife. '''\n",
        "\n",
        "text = ''.join(re.findall(f'[a-z,\\. ]', text.lower()))[:70]\n",
        "cesar = text2cesar(text, 4, dic)\n",
        "res = i2l(model_best(l2i(cesar, dic).to(dev)).argmax(dim=1).unsqueeze(0), dic)\n",
        "\n",
        "print(f'Исходный текст:                 {text}\\nЗашифрованный текст:            {cesar}\\nРасшифрованных моделью текст:   {res[0]}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Исходный текст:                 las vegas last april, john took a trip to las vegas, nevada. las vegas\n",
            "Зашифрованный текст:            pewbzikewbpewxbetvmpcbnslrbxssobebxvmtbxsbpewbzikewcbrizehedbpewbzikew\n",
            "Расшифрованных моделью текст:   las vegas last april, john took a trip to las vegas, nevada. las vegas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hiBDv6m15SE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5xOobu1nt3"
      },
      "source": [
        "###Задание 2.\n",
        "\n",
        "Выполнить практическую работу из лекционного ноутбука.\n",
        "а) построить RNN-ячейку на основе полносвязных слоев\n",
        "б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46aKe-KSaZ4a",
        "outputId": "06dc0fbd-fafd-4be8-c2ae-9f9f15bc5e22"
      },
      "source": [
        "import platform\n",
        "if platform.system() == 'Linux':\n",
        "    path = '/content/drive/My Drive/Colab Notebooks/dll/'\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    path = \"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApWIQS4kmgT"
      },
      "source": [
        "def text2tensor(text_in_list, dic, max_letters):\n",
        "    x1 = tr.zeros(len(text_in_list), max_letters).long().to(dev)\n",
        "    for i0 in range(len(text_in_list)):\n",
        "        for i1 in range(max_letters):\n",
        "            if i1 < len(text_in_list[i0]): x1[i0, i1] = dic[text_in_list[i0][i1]]\n",
        "            else: break\n",
        "    return x1\n",
        "\n",
        "def tensor2text(tensor_text, dic):\n",
        "    adic = {v:k for k, v in dic.items()}\n",
        "    return [''.join([adic[l.item()] for l in ph]).replace('none', ' ') for ph in tensor_text]\n",
        "\n",
        "def generate_text(text, dic, model, max_letters):\n",
        "    x1 = text2tensor([text, ''], dic, max_letters)\n",
        "    x2 = x1.clone()\n",
        "    for i in range(len(text), max_letters):\n",
        "        pred = x2\n",
        "        pred = model(x2).argmax(dim=-1)\n",
        "        x2[0,i] = pred[0,i-1]\n",
        "    return list(zip(tensor2text(x1, dic), tensor2text(x2, dic)))[0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xddU76zKK6K"
      },
      "source": [
        "class rnn2(tr.nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, n_classes, num_layers, lin_hidden_size):\n",
        "        super().__init__()\n",
        "        self.emb = tr.nn.Embedding(n_classes, emb_size)\n",
        "        self.rnn = tr.nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.lin1 = tr.nn.Linear(hidden_size, lin_hidden_size)\n",
        "        self.relu = tr.nn.ReLU6()\n",
        "        self.lin2 = tr.nn.Linear(lin_hidden_size, n_classes)\n",
        "    def forward(self, input):\n",
        "        r = self.emb(input)\n",
        "        r = self.rnn(r)[0]\n",
        "        r = self.lin1(r)\n",
        "        r = self.relu(r)\n",
        "        r = self.lin2(r).squeeze()\n",
        "        return r"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZBM_aLn4eOU"
      },
      "source": [
        "ab = ['none'] + list('abcdefghijklmnopqrstuvwxyz ')\n",
        "dic = dict(zip(ab, range(len(ab))))\n",
        "\n",
        "data = pd.read_csv(path + 'data.csv')\n",
        "text = [''.join(re.findall(f'[a-z ]', t)) for t in data.normalized_text.to_list() if type(t) is str]\n",
        "max_letters = 100\n",
        "X = text2tensor(text, dic, max_letters)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHg3XOtHJtjB",
        "outputId": "06e7f094-49ef-4194-eeb7-f3c93a8ebfee"
      },
      "source": [
        "n_classes = len(dic)\n",
        "adic = {v:k for k, v in dic.items()}\n",
        "batch_size = 100\n",
        "n_iters = 10\n",
        "\n",
        "model = rnn2(n_classes, 600, n_classes, 4, 600).to(dev)\n",
        "criterion = tr.nn.CrossEntropyLoss()\n",
        "optimizer = tr.optim.AdamW(model.parameters())\n",
        "loss_best = 10**10\n",
        "for iter in range(n_iters):\n",
        "    for i in range(X.shape[0] // batch_size):\n",
        "        x = X[i:i + batch_size,  :-1].to(dev)\n",
        "        y = X[i:i + batch_size, 1:  ].to(dev)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred.reshape(-1, len(dic)), y.flatten()).to(dev)\n",
        "        if loss < loss_best: \n",
        "            model_best = copy.copy(model)\n",
        "            loss_best = loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 200 == 0: \n",
        "            text1 = [''.join([adic[l.item()] for l in ph]).replace('none', ' ') for ph in x]\n",
        "            pred1 = [''.join([adic[l.item()] for l in ph]).replace('none', ' ') for ph in model(x).argmax(dim=-1)]\n",
        "            print(f'iter {iter}, {i*batch_size} of {X.shape[0]} batch, loss = {loss}, loss_best = {loss_best}, text/pred = {text1[0][1:11]}/{pred1[0][0:10]}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, 0 of 10891 batch, loss = 3.3511617183685303, loss_best = 3.3511617183685303, text/pred = aggie look/          \n",
            "iter 1, 0 of 10891 batch, loss = 1.109015703201294, loss_best = 0.9409886002540588, text/pred = aggie look/od  n tonn\n",
            "iter 2, 0 of 10891 batch, loss = 0.9451143145561218, loss_best = 0.3898526430130005, text/pred = aggie look/oybit maok\n",
            "iter 3, 0 of 10891 batch, loss = 0.6164137721061707, loss_best = 0.05802692472934723, text/pred = aggie look/oybi  mhmk\n",
            "iter 4, 0 of 10891 batch, loss = 0.2883683145046234, loss_best = 0.042576853185892105, text/pred = aggie look/oygie mook\n",
            "iter 5, 0 of 10891 batch, loss = 0.1323661357164383, loss_best = 0.02646530605852604, text/pred = aggie look/oygie sook\n",
            "iter 6, 0 of 10891 batch, loss = 0.09390679001808167, loss_best = 0.024721361696720123, text/pred = aggie look/oygie iook\n",
            "iter 7, 0 of 10891 batch, loss = 0.08211067318916321, loss_best = 0.024012155830860138, text/pred = aggie look/oygie iook\n",
            "iter 8, 0 of 10891 batch, loss = 0.07706151157617569, loss_best = 0.023683851584792137, text/pred = aggie look/oygie iook\n",
            "iter 9, 0 of 10891 batch, loss = 0.07359006255865097, loss_best = 0.02357950434088707, text/pred = aggie look/oygie dook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mp-KX-rAuWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c613d5a6-c49e-497b-96dc-e92dc158a405"
      },
      "source": [
        "text = 'my name is '\n",
        "generate_text(text, dic, model_best, 100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('my name is                                                                                          ',\n",
              " 'my name is lisa simpson and i have a problem                                                        ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UBNv-4I1XeB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaKhwz521XYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EOLQrP-1XUO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP7WSLl41XQc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3b7PTN_1XMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7KjZJH61XIe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xRm8SE1XES"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFW7-5qz1XAJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zrQyj521W7p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-n-FtfK1W1h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-EskOB1WvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWB2LU931WrI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ux2glbo1Wmp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eVzT3P780Cou",
        "outputId": "4129e0c5-852e-405f-fdf8-ae49c03f14b1"
      },
      "source": [
        "# RNN своими руками (матрицами) просто попробовать\n",
        "\n",
        "tr.autograd.set_detect_anomaly(True)\n",
        "n_iter = 1000\n",
        "shift = 4\n",
        "seq_len = 10\n",
        "batch_size = 50\n",
        "emb_size = 3\n",
        "hidden_size = 20\n",
        "\n",
        "w_emb = tr.randn(n_classes, emb_size, requires_grad=True)#.to(dev)\n",
        "w_x = tr.randn(emb_size, hidden_size, requires_grad=True)#.to(dev)\n",
        "w_h = tr.randn(hidden_size, hidden_size, requires_grad=True)#.to(dev)\n",
        "w_o = tr.randn(hidden_size, n_classes, requires_grad=True)#.to(dev)\n",
        "bias_h = tr.randn(1, requires_grad=True)#.to(dev)\n",
        "bias_o = tr.randn(1, requires_grad=True)#.to(dev)\n",
        "\n",
        "h_n = tr.zeros(batch_size, seq_len + 1, hidden_size)#.to(dev)\n",
        "h_0 = tr.randn(batch_size, hidden_size)#.to(dev)\n",
        "h_n[:,0,:] = h_0\n",
        "criterion = tr.nn.CrossEntropyLoss()\n",
        "lr = 0.00000001\n",
        "loss_hist = []\n",
        "loss_best = tr.tensor([10])**10 #.to(dev)\n",
        "for _ in range(n_iter):\n",
        "    text = tr.randint(0, n_classes, (batch_size, seq_len))#.to(dev)\n",
        "    cesar = (text + shift) % n_classes\n",
        "    text_ohe = tr.nn.functional.one_hot(text, num_classes=n_classes).float()\n",
        "    input = text_ohe @ w_emb\n",
        "    \n",
        "    for i0 in range(batch_size):\n",
        "        for i1 in range(seq_len):\n",
        "            h_n[i0, i1 + 1, :] = input[i0, i1, :] @ w_x + h_n[i0, i1, :].clone() @ w_h + bias_h\n",
        "    output = h_n[:,1:,:] @ w_o + bias_o\n",
        "    loss = criterion(output.view(-1, n_classes), text.flatten())#.to(dev)\n",
        "    loss_hist.append(loss)\n",
        "    if loss < loss_best:\n",
        "        model_best = dict(zip(['w_x', 'w_h', 'w_o', 'w_emb', 'bias_h', 'bias_o'], copy.copy([w_x, w_h, w_o, w_emb, bias_h, bias_o])))\n",
        "        loss_best = loss\n",
        "    loss.backward(retain_graph=True) \n",
        "    if _ % 1 == 0:\n",
        "        print(f'iter = {_}, loss = {loss.item()}, loss_best = {loss_best.item()}')\n",
        "    w_x = (w_x.detach() - w_x.grad * lr).requires_grad_(True)\n",
        "    w_h = (w_h.detach() - w_h.grad * lr).requires_grad_(True)\n",
        "    w_o = (w_o.detach() - w_o.grad * lr).requires_grad_(True)\n",
        "    w_emb = (w_emb.detach() - w_emb.grad * lr).requires_grad_(True)\n",
        "    bias_h = (bias_h.detach() - bias_h.grad * lr).requires_grad_(True)\n",
        "    bias_o = (bias_o.detach() - bias_o.grad * lr).requires_grad_(True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter = 0, loss = 1145435.75, loss_best = 100\n",
            "iter = 1, loss = 1399454.375, loss_best = 100\n",
            "iter = 2, loss = 1359835.75, loss_best = 100\n",
            "iter = 3, loss = 1234873.625, loss_best = 100\n",
            "iter = 4, loss = 1270491.25, loss_best = 100\n",
            "iter = 5, loss = 1269473.625, loss_best = 100\n",
            "iter = 6, loss = 1226739.625, loss_best = 100\n",
            "iter = 7, loss = 1208563.875, loss_best = 100\n",
            "iter = 8, loss = 1028989.25, loss_best = 100\n",
            "iter = 9, loss = 983545.125, loss_best = 100\n",
            "iter = 10, loss = 1066233.75, loss_best = 100\n",
            "iter = 11, loss = 918147.1875, loss_best = 100\n",
            "iter = 12, loss = 961007.75, loss_best = 100\n",
            "iter = 13, loss = 931604.25, loss_best = 100\n",
            "iter = 14, loss = 922200.75, loss_best = 100\n",
            "iter = 15, loss = 995915.875, loss_best = 100\n",
            "iter = 16, loss = 897117.0, loss_best = 100\n",
            "iter = 17, loss = 860088.875, loss_best = 100\n",
            "iter = 18, loss = 850170.3125, loss_best = 100\n",
            "iter = 19, loss = 849217.4375, loss_best = 100\n",
            "iter = 20, loss = 901752.1875, loss_best = 100\n",
            "iter = 21, loss = 975199.5, loss_best = 100\n",
            "iter = 22, loss = 902206.0625, loss_best = 100\n",
            "iter = 23, loss = 775680.5, loss_best = 100\n",
            "iter = 24, loss = 757654.0625, loss_best = 100\n",
            "iter = 25, loss = 803261.3125, loss_best = 100\n",
            "iter = 26, loss = 858525.25, loss_best = 100\n",
            "iter = 27, loss = 745504.3125, loss_best = 100\n",
            "iter = 28, loss = 750766.0, loss_best = 100\n",
            "iter = 29, loss = 704758.9375, loss_best = 100\n",
            "iter = 30, loss = 734268.0625, loss_best = 100\n",
            "iter = 31, loss = 769444.6875, loss_best = 100\n",
            "iter = 32, loss = 781994.0625, loss_best = 100\n",
            "iter = 33, loss = 654183.3125, loss_best = 100\n",
            "iter = 34, loss = 612975.625, loss_best = 100\n",
            "iter = 35, loss = 580049.6875, loss_best = 100\n",
            "iter = 36, loss = 677358.375, loss_best = 100\n",
            "iter = 37, loss = 661129.5625, loss_best = 100\n",
            "iter = 38, loss = 657376.6875, loss_best = 100\n",
            "iter = 39, loss = 579933.875, loss_best = 100\n",
            "iter = 40, loss = 631835.875, loss_best = 100\n",
            "iter = 41, loss = 599683.625, loss_best = 100\n",
            "iter = 42, loss = 609251.9375, loss_best = 100\n",
            "iter = 43, loss = 650077.5, loss_best = 100\n",
            "iter = 44, loss = 568053.0625, loss_best = 100\n",
            "iter = 45, loss = 632067.9375, loss_best = 100\n",
            "iter = 46, loss = 535777.375, loss_best = 100\n",
            "iter = 47, loss = 547528.9375, loss_best = 100\n",
            "iter = 48, loss = 623759.6875, loss_best = 100\n",
            "iter = 49, loss = 555307.75, loss_best = 100\n",
            "iter = 50, loss = 546324.0625, loss_best = 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0c9050723428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w_h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w_o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w_emb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bias_h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bias_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_o\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iter = {_}, loss = {loss.item()}, loss_best = {loss_best.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TePVLgKvTTyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636c070e-3ad6-44b6-8ae1-2139ecfe211a"
      },
      "source": [
        "text = '''Las Vegas Last April, John took a trip to Las Vegas, Nevada. \n",
        "Las Vegas is a popular destination in the western portion of the United States. \n",
        "The town is most popular for its casinos, hotels, and exciting nightlife. '''\n",
        "\n",
        "text_org = [''.join(re.findall(f'[a-z ]', text.lower()))[:70]]\n",
        "text = text2tensor(text_org, dic, seq_len)\n",
        "text_ohe = tr.nn.functional.one_hot(text, num_classes=n_classes).float().cpu()\n",
        "input = text_ohe @ w_emb\n",
        "batch_size = len(text)\n",
        "seq_len = len(text[0])\n",
        "hidden_size = 20\n",
        "\n",
        "h_n = tr.zeros(batch_size, seq_len + 1, hidden_size)\n",
        "h_0 = tr.randn(batch_size, hidden_size)\n",
        "h_n[:,0,:] = h_0\n",
        "for i0 in range(batch_size):\n",
        "    for i1 in range(seq_len):\n",
        "        # h_n[i0, i1 + 1, :] = input[i0, i1, :] @ model_best['w_x'] + h_n[i0, i1, :].clone() @ model_best['w_h'] + model_best['bias_h']\n",
        "        h_n[i0, i1 + 1, :] = input[i0, i1, :] @ w_x + h_n[i0, i1, :].clone() @ w_h + bias_h\n",
        "# output = h_n[:,1:,:] @ model_best['w_o'] + model_best['bias_o']\n",
        "output = h_n[:,1:,:] @ w_o + bias_o\n",
        "cesar = tensor2text((text + shift) % n_classes , dic)\n",
        "res = tensor2text(output.argmax(dim=-1), dic)\n",
        "\n",
        "print(f'Исходный текст:                 {tensor2text(text, dic)}\\nЗашифрованный текст:            {cesar}\\nРасшифрованных моделью текст:   {res[0]}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Исходный текст:                 ['las vegas last april john took a trip to las vegas nevada las vegas is']\n",
            "Зашифрованный текст:            ['pewbzikewbpewxbetvmpbnslrbxssobebxvmtbxsbpewbzikewbrizehebpewbzikewbmw']\n",
            "Расшифрованных моделью текст:   lnrd dlonvihod dhojhuhodhdhodhuhodhdhodhuhodhuhodhuhodhuhodhuhodhu    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWN98STFlJb0"
      },
      "source": [
        "# обучилась не оч"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}