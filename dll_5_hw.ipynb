{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dll.5.hw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyUdod/aml/blob/master/dll_5_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlkbYI-aq3h_"
      },
      "source": [
        "Задание 1.\n",
        "Обучите нейронную сеть решать шифр цезаря.\n",
        "Что надо сделать:\n",
        "1.Написать алгоритм шифра цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
        "2.Сделать нейронную сеть\n",
        "3.Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
        "4.Проверить качество\n",
        "\n",
        "Задание 2.\n",
        "Выполнить практическую работу из лекционного ноутбука.\n",
        "а) построить RNN-ячейку на основе полносвязных слоев\n",
        "б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXPes88uq3ir"
      },
      "source": [
        "import torch as tr\n",
        "import re\n",
        "from itertools import product\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "t = tr.tensor"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4oNmJgnP2Xj",
        "outputId": "aa2fbb81-205b-4869-e392-7505dccf9559"
      },
      "source": [
        "dev = tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
        "if dev.type == 'cuda':\n",
        "    print(tr.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(tr.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(tr.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
        "else:\n",
        "    print('CPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDlUKrd1HRv"
      },
      "source": [
        "def i2l(text, dic): \n",
        "    return [''.join([dic[letter.item()] for letter in sentense]) for sentense in text]\n",
        "\n",
        "def l2i(text, dic): \n",
        "    adic = {v:k  for k, v in dic.items()}\n",
        "    return tr.tensor([[adic[letter] for letter in sentense] for sentense in text])\n",
        "\n",
        "def text2cesar(text, shift, dic):\n",
        "    n_classes = len(dic)\n",
        "    return ''.join(i2l((l2i(text,dic) + shift) % n_classes, dic))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuQ-A2SaH8Uq"
      },
      "source": [
        "class rnn1(tr.nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, n_classes, num_layers):\n",
        "        super().__init__()\n",
        "        self.emb = tr.nn.Embedding(n_classes, emb_size)\n",
        "        self.rnn = tr.nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.lin = tr.nn.Linear(hidden_size, n_classes)\n",
        "        self.hidden_size = hidden_size\n",
        "    def forward(self, input):\n",
        "        r = self.emb(input)\n",
        "        r = self.rnn(r)[0]\n",
        "        r = self.lin(r).squeeze()\n",
        "        return r"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC_U8UNFO75T",
        "outputId": "467262f2-0d51-4f01-93d8-3ec6265b504e"
      },
      "source": [
        "ab = 'abcdefghijklmnopqrstuvwxyz ,.'\n",
        "dic = dict(zip(range(len(ab)), ab))\n",
        "\n",
        "n_classes = len(dic)\n",
        "cesar_shift = 4\n",
        "seq_len = 1000\n",
        "batch_size = 1\n",
        "n_iters = 1000\n",
        "\n",
        "model = rnn1(30, 100, len(dic), 1).to(dev)\n",
        "criterion = tr.nn.CrossEntropyLoss()\n",
        "optimizer = tr.optim.Adam(model.parameters())\n",
        "loss_best = 10**10\n",
        "for iter in range(n_iters):\n",
        "    text = tr.randint(0, n_classes, (batch_size, seq_len)).to(dev)\n",
        "    cesar = (text + cesar_shift) % n_classes\n",
        "    # optimizer.zero_grad()\n",
        "    pred = model(cesar)\n",
        "    loss = criterion(pred, text.flatten())#.to(dev)\n",
        "    if loss < loss_best: \n",
        "        model_best = copy.copy(model)\n",
        "        loss_best = loss\n",
        "    if iter % (n_iters / 10) == 0: \n",
        "        print(f'iter {iter} loss = {loss} ')\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 loss = 3.364370346069336 \n",
            "iter 100 loss = 0.046140220016241074 \n",
            "iter 200 loss = 0.002146678976714611 \n",
            "iter 300 loss = 0.0004706206964328885 \n",
            "iter 400 loss = 0.0005836443742737174 \n",
            "iter 500 loss = 0.00025856198044493794 \n",
            "iter 600 loss = 7.28902014088817e-05 \n",
            "iter 700 loss = 0.001037787296809256 \n",
            "iter 800 loss = 0.03093920461833477 \n",
            "iter 900 loss = 0.018807673826813698 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWMw4r1qeIf8",
        "outputId": "eca1bda0-4c79-44f3-ff4d-68c766fd8c5c"
      },
      "source": [
        "loss_best"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1399e-05, device='cuda:0', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H8nlzR6GFYo",
        "outputId": "8cc967dd-6838-42b5-eb74-fd5289ccf02d"
      },
      "source": [
        "text = '''Las Vegas Last April, John took a trip to Las Vegas, Nevada. \n",
        "Las Vegas is a popular destination in the western portion of the United States. \n",
        "The town is most popular for its casinos, hotels, and exciting nightlife. '''\n",
        "\n",
        "text = ''.join(re.findall(f'[a-z,\\. ]', text.lower()))[:70]\n",
        "cesar = text2cesar(text, 4, dic)\n",
        "res = i2l(model(l2i(cesar, dic).to(dev)).argmax(dim=1).unsqueeze(0), dic)\n",
        "\n",
        "print(f'Исходный текст:                 {text}\\nЗашифрованный текст:            {cesar}\\nРасшифрованных моделью текст:   {res[0]}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Исходный текст:                 las vegas last april, john took a trip to las vegas, nevada. las vegas\n",
            "Зашифрованный текст:            pewbzikewbpewxbetvmpcbnslrbxssobebxvmtbxsbpewbzikewcbrizehedbpewbzikew\n",
            "Расшифрованных моделью текст:   las vegas last april, john took a trip to las vegas, nevada. las vegas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWIcxfbLQto5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApNZDV8KQtd6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fNN2jTCQtOR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rppnf9p9PhR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjBjU-GWUSns"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "eVzT3P780Cou",
        "outputId": "a99002c9-d9ad-45a6-9d6e-b528b4c73636"
      },
      "source": [
        "tr.autograd.set_detect_anomaly(True)\n",
        "n_iter = 1000\n",
        "shift = 4\n",
        "seq_len = 10\n",
        "batch_size = 50\n",
        "emb_size = 3\n",
        "hidden_size = 20\n",
        "\n",
        "w_emb = tr.randn(n_classes, emb_size, requires_grad=True).to(dev)\n",
        "w_x = tr.randn(emb_size, hidden_size, requires_grad=True).to(dev)\n",
        "w_h = tr.randn(hidden_size, hidden_size, requires_grad=True).to(dev)\n",
        "w_o = tr.randn(hidden_size, n_classes, requires_grad=True).to(dev)\n",
        "bias_h = tr.randn(1, requires_grad=True).to(dev)\n",
        "bias_o = tr.randn(1, requires_grad=True).to(dev)\n",
        "\n",
        "h_n = tr.zeros(batch_size, seq_len + 1, hidden_size).to(dev)\n",
        "h_0 = tr.randn(batch_size, hidden_size).to(dev)\n",
        "h_n[:,0,:] = h_0\n",
        "criterion = tr.nn.CrossEntropyLoss()\n",
        "lr = 0.000000001\n",
        "loss_hist = []\n",
        "loss_best = tr.tensor([10]).to(dev)**10\n",
        "for _ in range(n_iter):\n",
        "    text = tr.randint(0, n_classes, (batch_size, seq_len)).to(dev)\n",
        "    cesar = (text + shift) % n_classes\n",
        "    text_ohe = tr.nn.functional.one_hot(text, num_classes=n_classes).float()\n",
        "    input = text_ohe @ w_emb\n",
        "    \n",
        "    for i0 in range(batch_size):\n",
        "        for i1 in range(seq_len):\n",
        "            h_n[i0, i1 + 1, :] = input[i0, i1, :] @ w_x + h_n[i0, i1, :].clone() @ w_h + bias_h\n",
        "    # h_n = h_n[:,1:,:]\n",
        "    output = h_n[:,1:,:] @ w_o + bias_o\n",
        "    loss = criterion(output.view(-1, n_classes), text.flatten())\n",
        "    loss_hist.append(loss)\n",
        "    if loss < loss_best:\n",
        "        model_best = dict(zip(['w_x', 'w_h', 'w_o', 'w_emb', 'bias_h', 'bias_o'], copy.copy([w_x, w_h, w_o, w_emb, bias_h, bias_o])))\n",
        "        loss_best = loss\n",
        "    loss.backward(retain_graph=True) \n",
        "    if _ % 10 == 0:\n",
        "        print(f'iter = {_}, loss = {loss.item()}, loss_best = {loss_best.item()}')\n",
        "    w_x = (w_x.detach() - w_x.grad * lr).requires_grad_(True)\n",
        "    w_h = (w_h.detach() - w_h.grad * lr).requires_grad_(True)\n",
        "    w_o = (w_o.detach() - w_o.grad * lr).requires_grad_(True)\n",
        "    w_emb = (w_emb.detach() - w_emb.grad * lr).requires_grad_(True)\n",
        "    bias_h = (bias_h.detach() - bias_h.grad * lr).requires_grad_(True)\n",
        "    bias_o = (bias_o.detach() - bias_o.grad * lr).requires_grad_(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter = 0, loss = 3272192.0, loss_best = 3272192.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-dcca9d2cc6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iter = {_}, loss = {loss.item()}, loss_best = {loss_best.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mw_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mw_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mw_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TePVLgKvTTyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9417bc-4610-41eb-d696-77ad7e017e08"
      },
      "source": [
        "text = '''Las Vegas Last April, John took a trip to Las Vegas, Nevada. \n",
        "Las Vegas is a popular destination in the western portion of the United States. \n",
        "The town is most popular for its casinos, hotels, and exciting nightlife. '''\n",
        "\n",
        "text = [''.join(re.findall(f'[a-z,\\. ]', text.lower()))[:70]]\n",
        "text_ohe = tr.nn.functional.one_hot(l2i(text, dic), num_classes=n_classes).float()\n",
        "input = text_ohe @ w_emb\n",
        "# cesar = (text + shift) % n_classes\n",
        "batch_size = len(text)\n",
        "seq_len = len(text[0])\n",
        "# emb_size = 3\n",
        "hidden_size = 20\n",
        "\n",
        "h_n = tr.zeros(batch_size, seq_len + 1, hidden_size)\n",
        "h_0 = tr.randn(batch_size, hidden_size)\n",
        "h_n[:,0,:] = h_0\n",
        "for i0 in range(batch_size):\n",
        "    for i1 in range(seq_len):\n",
        "        h_n[i0, i1 + 1, :] = input[i0, i1, :] @ model_best['w_x'] + h_n[i0, i1, :].clone() @ model_best['w_h'] + model_best['bias_h']\n",
        "output = h_n[:,1:,:] @ model_best['w_o'] + model_best['bias_o']\n",
        "\n",
        "\n",
        "cesar = text2cesar(text, 4, dic)\n",
        "res = i2l(output.argmax(dim=-1),dic)\n",
        "\n",
        "print(f'Исходный текст:                 {text}\\nЗашифрованный текст:            {cesar}\\nРасшифрованных моделью текст:   {res[0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Исходный текст:                 ['las vegas last april, john took a trip to las vegas, nevada. las vegas']\n",
            "Зашифрованный текст:            pewbzikewbpewxbetvmpcbnslrbxssobebxvmtbxsbpewbzikewcbrizehedbpewbzikew\n",
            "Расшифрованных моделью текст:   idiltohn didicqcoqrdmdxcntopidicqcnicqrdxcxobiouobio,idicxrdxoxodiouio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On-7RXhkqBVW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exWPIzJbDQJn"
      },
      "source": [
        ""
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46aKe-KSaZ4a",
        "outputId": "6465e08b-4ad0-49e1-cbb6-ca77ea1a6471"
      },
      "source": [
        "import platform\n",
        "if platform.system() == 'Linux':\n",
        "    path = '/content/drive/My Drive/Colab Notebooks/dll/'\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    path = \"\"#\"d:\\drive\\Colab Notebooks\\dll\\hymenoptera_data\\\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApWIQS4kmgT"
      },
      "source": [
        "def text2tensor(text_in_list, dic, max_letters):\n",
        "    # text1 = ['like what ','']\n",
        "    x1 = tr.zeros(len(text_in_list), max_letters).long().to(dev)\n",
        "    for i0 in range(len(text_in_list)):\n",
        "        for i1 in range(max_letters):\n",
        "            if i1 < len(text_in_list[i0]): x1[i0, i1] = dic[text_in_list[i0][i1]]\n",
        "            else: break\n",
        "    return x1\n",
        "\n",
        "def tensor2text(tensor_text, dic):\n",
        "    adic = {v:k for k, v in dic.items()}\n",
        "    return [''.join([adic[l.item()] for l in ph]).replace('none', ' ') for ph in tensor_text]\n",
        "\n",
        "def generate_text(text, dic, model, max_letters):\n",
        "    x1 = text2tensor([text, ''], dic, max_letters)\n",
        "    x2 = x1.clone()\n",
        "    for i in range(len(text), max_letters):\n",
        "        pred = x2\n",
        "        pred = model(x2).argmax(dim=-1)\n",
        "        x2[0,i] = pred[0,i-1]\n",
        "    return list(zip(tensor2text(x1, dic), tensor2text(x2, dic)))[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xddU76zKK6K"
      },
      "source": [
        "class rnn2(tr.nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, n_classes, num_layers, lin_hidden_size):\n",
        "        super().__init__()\n",
        "        self.emb = tr.nn.Embedding(n_classes, emb_size)\n",
        "        self.rnn = tr.nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.lin1 = tr.nn.Linear(hidden_size, lin_hidden_size)\n",
        "        self.relu = tr.nn.ReLU6()\n",
        "        self.lin2 = tr.nn.Linear(lin_hidden_size, n_classes)\n",
        "        # self.hidden_size = hidden_size\n",
        "    def forward(self, input):\n",
        "        r = self.emb(input)\n",
        "        r = self.rnn(r)[0]\n",
        "        r = self.lin1(r)\n",
        "        r = self.relu(r)\n",
        "        r = self.lin2(r).squeeze()\n",
        "        return r"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDaqb6tyuuVm",
        "outputId": "043010c4-09af-439b-b0bd-26144a9da3c9"
      },
      "source": [
        "data.normalized_text"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                   maggie look whats that\n",
              "1                                          lee-mur lee-mur\n",
              "2                                          zee-boo zee-boo\n",
              "3        im trying to teach maggie that nature doesnt e...\n",
              "4        its like an ox only it has a hump and a dewlap...\n",
              "                               ...                        \n",
              "11634    too bad we didnt come dressed as popular carto...\n",
              "11635    yeah mom guess what for a dollar a man sold me...\n",
              "11636                                   hows it going bart\n",
              "11637    maybe you need to play on their sympathies mor...\n",
              "11638                          ah ha now you look pathetic\n",
              "Name: normalized_text, Length: 11639, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZBM_aLn4eOU"
      },
      "source": [
        "ab = ['none'] + list('abcdefghijklmnopqrstuvwxyz ')\n",
        "dic = dict(zip(ab, range(len(ab))))\n",
        "\n",
        "data = pd.read_csv(path + 'data.csv')\n",
        "text = [''.join(re.findall(f'[a-z ]', t)) for t in data.normalized_text.to_list() if type(t) is str]\n",
        "max_letters = 100\n",
        "X = text2tensor(text, dic, max_letters)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHg3XOtHJtjB",
        "outputId": "d7b8b0b6-a994-4468-a212-93f2b2d48d6e"
      },
      "source": [
        "n_classes = len(dic)\n",
        "adic = {v:k for k, v in dic.items()}\n",
        "batch_size = 100\n",
        "n_iters = 10\n",
        "\n",
        "model = rnn2(n_classes, 600, n_classes, 4, 600).to(dev)\n",
        "criterion = tr.nn.CrossEntropyLoss()\n",
        "optimizer = tr.optim.AdamW(model.parameters())\n",
        "loss_best = 10**10\n",
        "for iter in range(n_iters):\n",
        "    for i in range(X.shape[0] // batch_size):\n",
        "        x = X[i:i + batch_size,  :-1].to(dev)\n",
        "        y = X[i:i + batch_size, 1:  ].to(dev)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred.reshape(-1, len(dic)), y.flatten()).to(dev)\n",
        "        if loss < loss_best: \n",
        "            model_best = copy.copy(model)\n",
        "            loss_best = loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 200 == 0: \n",
        "            text1 = [''.join([adic[l.item()] for l in ph]).replace('none', ' ') for ph in x]\n",
        "            pred1 = [''.join([adic[l.item()] for l in ph]).replace('none', ' ') for ph in model(x).argmax(dim=-1)]\n",
        "            print(f'iter {iter}, {i*batch_size} of {X.shape[0]} batch, loss = {loss}, loss_best = {loss_best}, text/pred = {text1[0][1:11]}/{pred1[0][0:10]}')"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, 0 of 10891 batch, loss = 3.3325107097625732, loss_best = 3.3325107097625732, text/pred = aggie look/          \n",
            "iter 1, 0 of 10891 batch, loss = 1.1262502670288086, loss_best = 0.9618717432022095, text/pred = aggie look/hreintaint\n",
            "iter 2, 0 of 10891 batch, loss = 0.9592496156692505, loss_best = 0.5594141483306885, text/pred = aggie look/oyeie sark\n",
            "iter 3, 0 of 10891 batch, loss = 0.7352344393730164, loss_best = 0.13298413157463074, text/pred = aggie look/oykie mumk\n",
            "iter 4, 0 of 10891 batch, loss = 0.289286345243454, loss_best = 0.03496379032731056, text/pred = aggie look/oygie mumk\n",
            "iter 5, 0 of 10891 batch, loss = 0.14930783212184906, loss_best = 0.02642136439681053, text/pred = aggie look/oygie mumk\n",
            "iter 6, 0 of 10891 batch, loss = 0.10438606142997742, loss_best = 0.0244036465883255, text/pred = aggie look/oygie mook\n",
            "iter 7, 0 of 10891 batch, loss = 0.08885058760643005, loss_best = 0.024079686030745506, text/pred = aggie look/oygie nook\n",
            "iter 8, 0 of 10891 batch, loss = 0.08013807982206345, loss_best = 0.023749947547912598, text/pred = aggie look/oygie look\n",
            "iter 9, 0 of 10891 batch, loss = 0.07457928359508514, loss_best = 0.023471485823392868, text/pred = aggie look/oygie look\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mp-KX-rAuWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9d5323-807f-47cb-bd0a-70bd33845c58"
      },
      "source": [
        "text = 'my name is '\n",
        "generate_text(text, dic, model_best, 100)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('my name is                                                                                          ',\n",
              " 'my name is lisa simpson and i have a problem                                                        ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    }
  ]
}